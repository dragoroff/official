{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/olegbrusilovski/.keras/datasets/spa-eng.zip\n"
     ]
    }
   ],
   "source": [
    "print(path_to_zip)\n",
    "path_to_file = os.path.dirname(path_to_zip) + '/spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "    return  word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(\" \"))\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples):\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "    \n",
    "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    # Spanish sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(\" \")] for en, sp in pairs]\n",
    "\n",
    "    # English sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(\" \")] for en, sp in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_targ = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=max_length_inp, padding='post')\n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen=max_length_targ, padding='post')\n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    5, 24107,     3,     4,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95171, 95171, 23793, 23793)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BATCH = BUFFER_SIZE // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "print(N_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9415 4936\n"
     ]
    }
   ],
   "source": [
    "vocab_inp_size = len(inp_lang.word2idx)+1\n",
    "vocab_tar_size = len(targ_lang.word2idx)+1\n",
    "print(vocab_inp_size, vocab_tar_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: ((16,), (11,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 16), (64, 11)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using Bahdanau attention. Lets decide on notation before writing the simplified form:\n",
    "\n",
    "# FC = Fully connected (dense) layer\n",
    "# EO = Encoder output\n",
    "# H = hidden state\n",
    "# X = input to the decoder\n",
    "# And the pseudo-code:\n",
    "\n",
    "# score = FC(tanh(FC(EO) + FC(H)))\n",
    "# attention weights = softmax(score, axis = 1). Softmax by default is applied on the last axis but here we want to apply\n",
    "# it on the 1st axis, since the shape of score is (batch_size, max_length, 1). Max_length is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "# context vector = sum(attention weights * EO, axis = 1). Same reason as above for choosing axis as 1.\n",
    "# embedding output = The input to the decoder X is passed through an embedding layer.\n",
    "# merged vector = concat(embedding output, context vector)\n",
    "# This merged vector is then given to the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, return_sequences=True, return_state=True, \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state  = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super().__init__()\n",
    "        self.batch_sz  =  batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
    "# 2.The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
    "# 3.The decoder returns the predictions and the decoder hidden state.\n",
    "# 4.The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "# 5.Use teacher forcing to decide the next input to the decoder.\n",
    "# 6.Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
    "# 7.The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5, 1857,    3, ...,    0,    0,    0],\n",
       "       [   5, 1857,    3, ...,    0,    0,    0],\n",
       "       [   5, 1857,    3, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   5, 4760, 2555, ...,    0,    0,    0],\n",
       "       [   5, 4760, 2555, ...,    0,    0,    0],\n",
       "       [   5, 4760, 2555, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5, 9090,    3, ...,    0,    0,    0],\n",
       "       [   5, 9204,    3, ...,    0,    0,    0],\n",
       "       [   5, 9082,    3, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   5, 5498, 6764, ...,    0,    0,    0],\n",
       "       [   5, 6228, 3454, ...,    0,    0,    0],\n",
       "       [   5, 8600, 9184, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 16), (64, 11)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 11)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_lang.word2idx['<start>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6272\n",
      "Epoch 1 Batch 100 Loss 2.0286\n",
      "Epoch 1 Batch 200 Loss 1.8551\n",
      "Epoch 1 Batch 300 Loss 1.7520\n",
      "Epoch 1 Loss 2.0705\n",
      "Time taken for 1 epoch 604.2985301017761 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7190\n",
      "Epoch 2 Batch 100 Loss 1.4534\n",
      "Epoch 2 Batch 200 Loss 1.4463\n",
      "Epoch 2 Batch 300 Loss 1.3797\n",
      "Epoch 2 Loss 1.4951\n",
      "Time taken for 1 epoch 613.4460883140564 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.3666\n",
      "Epoch 3 Batch 100 Loss 1.1136\n",
      "Epoch 3 Batch 200 Loss 1.1388\n",
      "Epoch 3 Batch 300 Loss 0.9926\n",
      "Epoch 3 Loss 1.1394\n",
      "Time taken for 1 epoch 618.4113059043884 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.9930\n",
      "Epoch 4 Batch 100 Loss 0.7832\n",
      "Epoch 4 Batch 200 Loss 0.7893\n",
      "Epoch 4 Batch 300 Loss 0.7083\n",
      "Epoch 4 Loss 0.8100\n",
      "Time taken for 1 epoch 632.605140209198 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.6823\n",
      "Epoch 5 Batch 100 Loss 0.5741\n",
      "Epoch 5 Batch 200 Loss 0.5591\n",
      "Epoch 5 Batch 300 Loss 0.5036\n",
      "Epoch 5 Loss 0.5637\n",
      "Time taken for 1 epoch 632.4494869709015 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.4837\n",
      "Epoch 6 Batch 100 Loss 0.4213\n",
      "Epoch 6 Batch 200 Loss 0.4013\n",
      "Epoch 6 Batch 300 Loss 0.3738\n",
      "Epoch 6 Loss 0.3923\n",
      "Time taken for 1 epoch 652.1697919368744 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.3434\n",
      "Epoch 7 Batch 100 Loss 0.3341\n",
      "Epoch 7 Batch 200 Loss 0.3025\n",
      "Epoch 7 Batch 300 Loss 0.2894\n",
      "Epoch 7 Loss 0.2803\n",
      "Time taken for 1 epoch 638.534590959549 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.2338\n",
      "Epoch 8 Batch 100 Loss 0.2440\n",
      "Epoch 8 Batch 200 Loss 0.2239\n",
      "Epoch 8 Batch 300 Loss 0.1963\n",
      "Epoch 8 Loss 0.2031\n",
      "Time taken for 1 epoch 624.9335532188416 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1923\n",
      "Epoch 9 Batch 100 Loss 0.1871\n",
      "Epoch 9 Batch 200 Loss 0.1654\n",
      "Epoch 9 Batch 300 Loss 0.1540\n",
      "Epoch 9 Loss 0.1541\n",
      "Time taken for 1 epoch 603.5670330524445 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1254\n",
      "Epoch 10 Batch 100 Loss 0.1407\n",
      "Epoch 10 Batch 200 Loss 0.1296\n",
      "Epoch 10 Batch 300 Loss 0.1046\n",
      "Epoch 10 Loss 0.1186\n",
      "Time taken for 1 epoch 609.440633058548 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)\n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                loss += loss_function(targ[:, t] ,predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        total_loss += batch_loss\n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(\" \")]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding=\"post\")\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['start']], 0)\n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.idx2word[predicted_id] + \" \"\n",
    "        \n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15c7039e8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> voy a ir al cine esta noche . <end>\n",
      "Predicted translation: i often tonight . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'voy a ir al cine esta noche.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
